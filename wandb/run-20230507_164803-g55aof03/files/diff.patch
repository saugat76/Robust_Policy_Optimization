diff --git a/rpo_continuous_action.py b/rpo_continuous_action.py
index 14fb922..7c14c73 100644
--- a/rpo_continuous_action.py
+++ b/rpo_continuous_action.py
@@ -74,7 +74,7 @@ def parse_args():
         help="the alpha parameter for RPO")
     parser.add_argument("--num-nodes", type=float, default=64,
         help="number of nodes used for actor and critic n/w")
-    parser.add_argument("--activation-func", type=str, default="Tanh",
+    parser.add_argument("--activation-func", type=str, default="nn.Tanh()",
         help="activation function used in actor and critic n/w: 'nn.ReLu()', 'nn.Tanh()', 'nn.Sigmoid()'")
     
     
@@ -119,20 +119,28 @@ class Agent(nn.Module):
     def __init__(self, envs, rpo_alpha, num_nodes, activation_func):
         super().__init__()
         self.rpo_alpha = rpo_alpha
-        # Setting of the nodes used in actor and critic network nodes as a sys args variable
         # Changed by Saugat
+        # Setting of the nodes used in actor and critic network nodes as a sys args variable
+        # Since the functions are not serializable so, using if else to select the respective activation function
+        if activation_func  == "nn.ReLU()":
+            activation = nn.ReLU()
+        elif activation_func  == "nn.Tanh()":
+            activation = nn.Tanh()
+        elif activation_func == "nn.Sigmoid()":
+            activation = nn.Sigmoid() 
+
         self.critic = nn.Sequential(
             layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), num_nodes)),
-            activation_func,
+            activation,
             layer_init(nn.Linear(num_nodes, num_nodes)),
-            activation_func,
+            activation,
             layer_init(nn.Linear(num_nodes, 1), std=1.0),
         )
         self.actor_mean = nn.Sequential(
             layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), num_nodes)),
-            activation_func,
+            activation,
             layer_init(nn.Linear(num_nodes, num_nodes)),
-            activation_func,
+            activation,
             layer_init(nn.Linear(num_nodes, np.prod(envs.single_action_space.shape)), std=0.01),
         )
         self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
@@ -174,8 +182,8 @@ sweep_configuration = {
     'batch_size': {'values': [16, 32, 64]},
     'lr': {'max': 0.1, 'min': 0.0001},
     'num_nodes': {'values': [64, 128, 256]},
-    'optimizer_choices': {'values': ['Adam', 'RMSProp', 'AdamW']}, 
-    'activation_funcs': {'values': [nn.ReLu(), nn.Tanh(), nn.Sigmoid()]}
+    'optimizer_choices': {'values': ["Adam", "RMSProp", "AdamW"]}, 
+    'activation_funcs': {'values': ["ReLU()", "nn.Tanh()", "nn.Sigmoid()"]}
     
     }
 }
@@ -190,7 +198,7 @@ def main():
     # May use wandb to do hyperparameter tunning and evaluate the model performance 
     run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
     if args.track:
-        wandb.init(
+        run = wandb.init(
             project=args.wandb_project_name,
             entity=args.wandb_entity,
             sync_tensorboard=True,
@@ -202,7 +210,7 @@ def main():
         args.learning_rate = wandb.config.lr
         args.batch_size = wandb.config.batch_size
         args.num_nodes = wandb.config.num_nodes
-        optimizer_choice = wandb.config.optimizer
+        optimizer_choice = wandb.config.optimizer_choices
         args.activation_func = wandb.config.activation_funcs 
         
     writer = SummaryWriter(f"runs/{run_name}")
@@ -222,7 +230,9 @@ def main():
         [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
     )
     assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
+    
+    # Added by Saugat 
+    # Since the functions are not serializable so, using if else to select the respective activation function
     agent = Agent(envs, args.rpo_alpha, args.num_nodes, args.activation_func).to(device)
 
     # Added by Saugat 
@@ -400,10 +410,12 @@ def main():
                     video_filenames.add(filename)
 
     envs.close()
+    wandb.finish()
     writer.close()
 
 
 ## Addition by Saugat for implementation in notebook 
 # Sweep the code // main function usign the sweep configuration 
 # For the implemntation // running in CLI ->   wandb agent sweep_id
-# wandb.agent(sweep_id=sweep_id, function=main)
\ No newline at end of file
+# wandb.agent(sweep_id=sweep_id, function=main)
+wandb.agent(sweep_id, function=main, count=5)
\ No newline at end of file
